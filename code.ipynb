{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/media/popo/Elements/Datasets/Leaf-disease-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = json.load(open(f'{PATH}label_num_to_disease_map.json'))\n",
    "train_ids = pd.read_csv(f'{PATH}train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the distribution of the labels\n",
    "train_ids['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is class imbalance in the dataset, so we need to take care of it. We can use the stratified sampling method to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image dataset\n",
    "class LeafDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, img_label = self.df.iloc[idx]\n",
    "        img_label = torch.tensor(img_label)\n",
    "        # print(img_label)\n",
    "        # print(img_name)\n",
    "        img_path = os.path.join(self.path, img_name)\n",
    "        #print(img_path)\n",
    "        img = plt.imread(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'{PATH}train_images/'\n",
    "df = train_ids\n",
    "train_dataset = LeafDataset(df, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train and test  \n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Resnet34\n",
    "model = models.resnet34(pretrained=True)\n",
    "#change last layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(label_dict))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [50/134], Loss: 0.7863\n",
      "Epoch [1/10], Step [100/134], Loss: 0.5797\n",
      "Epoch [2/10], Step [50/134], Loss: 0.5349\n",
      "Epoch [2/10], Step [100/134], Loss: 0.5272\n",
      "Epoch [3/10], Step [50/134], Loss: 0.4564\n",
      "Epoch [3/10], Step [100/134], Loss: 0.3813\n",
      "Epoch [4/10], Step [50/134], Loss: 0.4831\n",
      "Epoch [4/10], Step [100/134], Loss: 0.4519\n",
      "Epoch [5/10], Step [50/134], Loss: 0.3021\n",
      "Epoch [5/10], Step [100/134], Loss: 0.4381\n",
      "Epoch [6/10], Step [50/134], Loss: 0.1662\n",
      "Epoch [6/10], Step [100/134], Loss: 0.3545\n",
      "Epoch [7/10], Step [50/134], Loss: 0.2174\n",
      "Epoch [7/10], Step [100/134], Loss: 0.2087\n",
      "Epoch [8/10], Step [50/134], Loss: 0.1096\n",
      "Epoch [8/10], Step [100/134], Loss: 0.1334\n",
      "Epoch [9/10], Step [50/134], Loss: 0.0627\n",
      "Epoch [9/10], Step [100/134], Loss: 0.1337\n",
      "Epoch [10/10], Step [50/134], Loss: 0.0877\n",
      "Epoch [10/10], Step [100/134], Loss: 0.0567\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8083148c435d231f703449331507e72b23a2d1daf1c5ab6243dbd5548bf4abd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
